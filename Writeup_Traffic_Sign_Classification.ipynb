{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic Sign Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goals / steps of this project are the following:\n",
    "\n",
    "    1. Load the data set (see below for links to the project data set)\n",
    "    2. Explore, summarize and visualize the data set\n",
    "    3. Design, train and test a model architecture\n",
    "    4. Use the model to make predictions on new images\n",
    "    5. Analyze the softmax probabilities of the new images\n",
    "    6. Summarize the results with a written report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set Summary & Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Provide a basic summary of the data set. In the code, the analysis should be done using python, numpy and/or pandas methods rather than hardcoding results manually.\n",
    "  The code for this step is contained in the IPython notebook under the heading \"Step 1: Dataset Summary &      Exploration\" and subsection of \"Provide a Basic Summary of the Data Set Using Python, Numpy and/or Pandas\"\n",
    "\n",
    "I used the numpy library to calculate summary statistics of the traffic signs data set:\n",
    "\n",
    "   1. The size of training set is **34799** \n",
    "   2. The size of the validation set is **12630**\n",
    "   3. The size of test set is **4410**\n",
    "   4. The shape of a traffic sign image is **(32, 32)** and number of channels is **3** (color)\n",
    "   5. The number of unique classes/labels in the data set is **43**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Include an exploratory visualization of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for this step is contained in the IPython notebook under the heading \"Include an exploratory visualization of the dataset\"\n",
    "\n",
    "Here is the list of classes and their indices:\n",
    "\n",
    "     0                                     Speed limit (20km/h)\n",
    "     1                                     Speed limit (30km/h)\n",
    "     2                                     Speed limit (50km/h)\n",
    "     3                                     Speed limit (60km/h)\n",
    "     4                                     Speed limit (70km/h)\n",
    "     5                                     Speed limit (80km/h)\n",
    "     6                              End of speed limit (80km/h)\n",
    "     7                                    Speed limit (100km/h)\n",
    "     8                                    Speed limit (120km/h)\n",
    "     9                                               No passing\n",
    "     10            No passing for vehicles over 3.5 metric tons\n",
    "     11                   Right-of-way at the next intersection\n",
    "     12                                           Priority road\n",
    "     13                                                   Yield\n",
    "     14                                                    Stop\n",
    "     15                                             No vehicles\n",
    "     16                Vehicles over 3.5 metric tons prohibited\n",
    "     17                                                No entry\n",
    "     18                                         General caution\n",
    "     19                             Dangerous curve to the left\n",
    "     20                            Dangerous curve to the right\n",
    "     21                                            Double curve\n",
    "     22                                              Bumpy road\n",
    "     23                                           Slippery road\n",
    "     24                               Road narrows on the right\n",
    "     25                                               Road work\n",
    "     26                                         Traffic signals\n",
    "     27                                             Pedestrians\n",
    "     28                                       Children crossing\n",
    "     29                                       Bicycles crossing\n",
    "     30                                      Beware of ice/snow\n",
    "     31                                   Wild animals crossing\n",
    "     32                     End of all speed and passing limits\n",
    "     33                                        Turn right ahead\n",
    "     34                                         Turn left ahead\n",
    "     35                                              Ahead only\n",
    "     36                                    Go straight or right\n",
    "     37                                     Go straight or left\n",
    "     38                                              Keep right\n",
    "     39                                               Keep left\n",
    "     40                                    Roundabout mandatory\n",
    "     41                                       End of no passing\n",
    "     42       End of no passing by vehicles over 3.5 metric ...\n",
    "\n",
    "\n",
    "Here I draw the last figure in each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./last_image_in_each_class.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and here is the distribution of dataset: Number of images in each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./hist_images.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design and Test a Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Describe how you preprocessed the image data. What techniques were chosen and why did you choose these techniques? Consider including images showing the output of each preprocessing technique. Pre-processing refers to techniques such as converting to grayscale, normalization, etc. (OPTIONAL: As described in the \"Stand Out Suggestions\" part of the rubric, if you generated additional data for training, describe why you decided to generate additional data, how you generated the data, and provide example images of the additional data. Then describe the characteristics of the augmented training set like number of images in the set, number of images for each class, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1.1 Data Augmentation:\n",
    "As it is obvious from figure above, there is a large variatoin in distribution of images per classes, some classes such as \"go straight or left\" have less than 200 images where others such as \"speed limit(30km/h) have close to 2000, so images clearly do not have a fair distribution of all types. I did augmentation, which added extra samples to the training set by applying different forms of transformations, including random rotation, x and y translation to the original image. At the end we have same number of images per classes(2000 images per class).\n",
    "\n",
    "###### 1.2 Convert to gray Scale\n",
    "I converted images from color scale to gray scale.\n",
    "\n",
    "###### 1.3 Normalization\n",
    "I normalized the image data to remove differences of contrast and brightness between images. Here normalizing is standardizing which is making the data zero-centered by subtracting its mean and normalizing it by its standard deviation to have a unit variance. \n",
    "\n",
    "Now, augmented images are generated using described pre-processed data(gray scale and normalized). In data augmentation, I used pre-defined and built-in functions in OpenCV (cv2.getRotationMatrix2D and cv2.warpAffine)\n",
    "\n",
    "\n",
    "After applying all type of pre-processing and data augmentaation, here is the summary of result:\n",
    "\n",
    "    Training Set:   129000 samples\n",
    "    Validation Set: 4410 samples\n",
    "    Test Set:       12630 samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now here is the distribution of dataset after preprocessing and augmentation: Number of images in each class\n",
    "![](./afterAug.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in above figure there is a fairly distribution among images in each classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Model Architecture\n",
    "Describe what your final model architecture looks like including model type, layers, layer sizes, connectivity, etc.) Consider including a diagram and/or table describing the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My final model consisted of the following layers:\n",
    "\n",
    "    Layer                       Description\n",
    "    ------------------------------------------------------------------------\n",
    "    Input\t                     32x32x1 GRAY scale image\n",
    "    Convolutional Layer1(5x5)\t 1x1 stride, Valid padding, Output = 28x28x6\n",
    "    Activation 1                  RELU\n",
    "    Max Pooling 1                 2x2 stride, Output = 14x14x6\n",
    "    ------------------------------------------------------------------------\n",
    "    Convolutional Layer2(5x5)\t 1x1 stride, Valid padding, Output = 10x10x16\n",
    "    Activation 2                  RELU\n",
    "    Max Pooling 2                 2x2 stride, Output = 5x5x16\n",
    "    ------------------------------------------------------------------------\n",
    "    Flatten\t                   Output = 400\n",
    "    Fully connected Layer1\t    Output = 120\n",
    "    Activation 3\t              RELU\n",
    "    Dropout 1\t                 Keep Probability = 0.7\n",
    "    ------------------------------------------------------------------------\n",
    "    Fully connected Layer2\t    Output = 84\n",
    "    Activation 4\t              RELU\n",
    "    Dropout 2\t                 Keep Probability = 0.7\n",
    "    ------------------------------------------------------------------------\n",
    "    Fully connected Layer 3\t   Output = 43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Training Model\n",
    "Describe how you trained your model. The discussion can include the type of optimizer, the batch size, number of epochs and any hyperparameters such as learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to tune the hyper parameters including:\n",
    "    1. Adam gradient optimizer learning rate: 0.0009\n",
    "    2. Regularization strength\n",
    "    3. Number of epochs: 60\n",
    "    4. Batch size: 128\n",
    "    5. Dropout probability: 0.7\n",
    "\n",
    "I used an Adam optimizer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Approach taken for finding a solution\n",
    "Describe the approach taken for finding a solution and getting the validation set accuracy to be at least 0.93. Include in the discussion the results on the training, validation and test sets and where in the code these were calculated. Your approach may have been an iterative process, in which case, outline the steps you took to get to the final solution and why you chose those steps. Perhaps your solution involved an already well known implementation or architecture. In this case, discuss why you think the architecture is suitable for the current problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### BASELINE Architecture (LeNet):\n",
    "\n",
    "I started the project (as mentioned by udacity) by running LeNet architecture on the German Traffic Sign Dataset. In order to do that I only modified the followings:\n",
    "\n",
    "    Changing the X_train shape such that it has 1 channel\n",
    "    Changing the number of classes from 10 in the MNIST dataset to 43 which is the length of the test set.\n",
    "    \n",
    "data is already splitted in to validation and training sets, so there is no need to manually doing this by using sklearn.model library. I assigned 20% of the data for validation and 80% is left for training. \n",
    "\n",
    "I chose the LeNet structure as the baseline of my network to classify traffic signs. It is simple and small enough to be trained quickly for making decisions regarding the image pre-processing and data augmentations.\n",
    "\n",
    "\"Here is the log of training:\"\n",
    "\n",
    "Training...\n",
    "\n",
    "EPOCH 1 ...\n",
    "Validation Accuracy = 0.822\n",
    "\n",
    "EPOCH 2 ...\n",
    "Validation Accuracy = 0.869\n",
    "\n",
    "EPOCH 3 ...\n",
    "Validation Accuracy = 0.890\n",
    "\n",
    "EPOCH 4 ...\n",
    "Validation Accuracy = 0.897\n",
    "\n",
    "EPOCH 5 ...\n",
    "Validation Accuracy = 0.910\n",
    "\n",
    "EPOCH 6 ...\n",
    "Validation Accuracy = 0.908\n",
    "\n",
    "EPOCH 7 ...\n",
    "Validation Accuracy = 0.912\n",
    "\n",
    "EPOCH 8 ...\n",
    "Validation Accuracy = 0.904\n",
    "\n",
    "EPOCH 9 ...\n",
    "Validation Accuracy = 0.921\n",
    "\n",
    "EPOCH 10 ...\n",
    "Validation Accuracy = 0.921\n",
    "\n",
    "EPOCH 11 ...\n",
    "Validation Accuracy = 0.923\n",
    "\n",
    "EPOCH 12 ...\n",
    "Validation Accuracy = 0.935\n",
    "\n",
    "EPOCH 13 ...\n",
    "Validation Accuracy = 0.933\n",
    "\n",
    "EPOCH 14 ...\n",
    "Validation Accuracy = 0.930\n",
    "\n",
    "EPOCH 15 ...\n",
    "Validation Accuracy = 0.939\n",
    "\n",
    "EPOCH 16 ...\n",
    "Validation Accuracy = 0.940\n",
    "\n",
    "EPOCH 17 ...\n",
    "Validation Accuracy = 0.921\n",
    "\n",
    "EPOCH 18 ...\n",
    "Validation Accuracy = 0.933\n",
    "\n",
    "EPOCH 19 ...\n",
    "Validation Accuracy = 0.939\n",
    "\n",
    "EPOCH 20 ...\n",
    "Validation Accuracy = 0.940\n",
    "\n",
    "EPOCH 21 ...\n",
    "Validation Accuracy = 0.930\n",
    "\n",
    "EPOCH 22 ...\n",
    "Validation Accuracy = 0.935\n",
    "\n",
    "EPOCH 23 ...\n",
    "Validation Accuracy = 0.934\n",
    "\n",
    "EPOCH 24 ...\n",
    "Validation Accuracy = 0.943\n",
    "\n",
    "EPOCH 25 ...\n",
    "Validation Accuracy = 0.940\n",
    "\n",
    "EPOCH 26 ...\n",
    "Validation Accuracy = 0.942\n",
    "\n",
    "EPOCH 27 ...\n",
    "Validation Accuracy = 0.941\n",
    "\n",
    "EPOCH 28 ...\n",
    "Validation Accuracy = 0.946\n",
    "\n",
    "EPOCH 29 ...\n",
    "Validation Accuracy = 0.952\n",
    "\n",
    "EPOCH 30 ...\n",
    "Validation Accuracy = 0.952\n",
    "\n",
    "EPOCH 31 ...\n",
    "Validation Accuracy = 0.948\n",
    "\n",
    "EPOCH 32 ...\n",
    "Validation Accuracy = 0.946\n",
    "\n",
    "EPOCH 33 ...\n",
    "Validation Accuracy = 0.937\n",
    "\n",
    "EPOCH 34 ...\n",
    "Validation Accuracy = 0.949\n",
    "\n",
    "EPOCH 35 ...\n",
    "Validation Accuracy = 0.949\n",
    "\n",
    "EPOCH 36 ...\n",
    "Validation Accuracy = 0.943\n",
    "\n",
    "EPOCH 37 ...\n",
    "Validation Accuracy = 0.957\n",
    "\n",
    "EPOCH 38 ...\n",
    "Validation Accuracy = 0.960\n",
    "\n",
    "EPOCH 39 ...\n",
    "Validation Accuracy = 0.954\n",
    "\n",
    "EPOCH 40 ...\n",
    "Validation Accuracy = 0.946\n",
    "\n",
    "EPOCH 41 ...\n",
    "Validation Accuracy = 0.947\n",
    "\n",
    "EPOCH 42 ...\n",
    "Validation Accuracy = 0.942\n",
    "\n",
    "EPOCH 43 ...\n",
    "Validation Accuracy = 0.955\n",
    "\n",
    "EPOCH 44 ...\n",
    "Validation Accuracy = 0.959\n",
    "\n",
    "EPOCH 45 ...\n",
    "Validation Accuracy = 0.946\n",
    "\n",
    "EPOCH 46 ...\n",
    "Validation Accuracy = 0.955\n",
    "\n",
    "EPOCH 47 ...\n",
    "Validation Accuracy = 0.939\n",
    "\n",
    "EPOCH 48 ...\n",
    "Validation Accuracy = 0.959\n",
    "\n",
    "EPOCH 49 ...\n",
    "Validation Accuracy = 0.956\n",
    "\n",
    "EPOCH 50 ...\n",
    "Validation Accuracy = 0.949\n",
    "\n",
    "EPOCH 51 ...\n",
    "Validation Accuracy = 0.952\n",
    "\n",
    "EPOCH 52 ...\n",
    "Validation Accuracy = 0.944\n",
    "\n",
    "EPOCH 53 ...\n",
    "Validation Accuracy = 0.949\n",
    "\n",
    "EPOCH 54 ...\n",
    "Validation Accuracy = 0.952\n",
    "\n",
    "EPOCH 55 ...\n",
    "Validation Accuracy = 0.956\n",
    "\n",
    "EPOCH 56 ...\n",
    "Validation Accuracy = 0.947\n",
    "\n",
    "EPOCH 57 ...\n",
    "Validation Accuracy = 0.944\n",
    "\n",
    "EPOCH 58 ...\n",
    "Validation Accuracy = 0.949\n",
    "\n",
    "EPOCH 59 ...\n",
    "Validation Accuracy = 0.939\n",
    "\n",
    "EPOCH 60 ...\n",
    "Validation Accuracy = 0.953\n",
    "\n",
    "Model saved\n",
    "In [26]:\n",
    "\n",
    "\n",
    "My final model results were:\n",
    "\n",
    "    training set accuracy of 99.9%\n",
    "    validation set accuracy of 95.3%\n",
    "    test set accuracy of 93.1%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If an iterative approach was chosen: What was the first architecture that was tried and why was it chosen?\n",
    "\n",
    "I chose the LeNet structure as the baseline of my network to classify traffic signs. It is simple and small enough to be trained quickly for making decisions regarding the image pre-processing and data augmentations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What were some problems with the initial architecture?\n",
    "The training data was able to achieve very good accuracy but validation data accuracy was 85%. So, it had overfitting problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How was the architecture adjusted and why was it adjusted? Typical adjustments could include choosing a different model architecture, adding or taking away layers (pooling, dropout, convolution, etc.), using an activation function or changing the activation function. One common justification for adjusting an architecture would be due to over fitting or under fitting. A high accuracy on the training set but low accuracy on the validation set indicates over fitting; a low accuracy on both sets indicates under fitting.\n",
    "\n",
    "To overcome over-fitting, I have added dropouts to the model at various layer and experimented different keep probabilities. The best result seem to be placing two dropouts between the three fully connected layer and keep probability of 0.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Which parameters were tuned? How were they adjusted and why?\n",
    "\n",
    "At first I chose a large value for learning rate to increase the speed of the training but the result was not good enough, so I lowered to small value.\n",
    "The keep_prob = 0.7 and was the best option, for data augmentation, I used two transformation of rotation and translation. At the end, I generated 3000 images for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What are some of the important design choices and why were they chosen? For example, why might a convolution layer work well with this problem? How might a dropout layer help with creating a successful model?\n",
    "\n",
    "I used the dropout layers in fully connected layers to help it to reduce over-fitting of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If a well known architecture was chosen: What architecture was chosen?\n",
    "\n",
    "The LeNet model with dropout layers is great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why did you believe it would be relevant to the traffic sign application?\n",
    "\n",
    "It has great and promising results on predicting 32x32x3 images on letters and numbers. I beleive recognise objects in similar size as street signs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How does the final model's accuracy on the training, validation and test set provide evidence that the model is working well?\n",
    "\n",
    "the accuracy on test data set is 95.3%. It is higher than the minimum requirement of what udacity asked(93%) for the validation set. However, the model is still over-fitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a Model on New Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Choose five German traffic signs found on the web and provide them in the report. For each image, discuss what quality or qualities might be difficult to classify.\n",
    "\n",
    "Here are five German traffic signs.\n",
    "\n",
    "first image:\n",
    "![](./new_images/image_1.png)\n",
    "\n",
    "The first image is a Priority road sign. The image is slightly blurry but through the program, the image will be clear and the model should work well.\n",
    "\n",
    "second image:\n",
    "![](./new_images/image_2.png)\n",
    "\n",
    "The second image is a Turn left ahead sign. The image is slightly blurry and tilted.\n",
    "\n",
    "third image:\n",
    "![](./new_images/image_3.png)\n",
    "\n",
    "The third image is a Speed limit (60km/h) sign. The lighting is certainly different.\n",
    "\n",
    "fourth image:\n",
    "![](./new_images/image_4.png)\n",
    "\n",
    "The fourth image is a Right-of-way at the next intersection sign. The lighting is certainly different.\n",
    "\n",
    "fifth image:\n",
    "![](./new_images/image_5.png)\n",
    "\n",
    "The fifth image is a Speed limit (20km/h) sign.\n",
    "\n",
    "\n",
    "I wrote some lines code to capture the class id of images as an input to my model so the accuracy is 100%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Discuss the model's predictions on these new traffic signs and compare the results to predicting on the test set. Identify where in your code predictions were made. At a minimum, discuss what the predictions were, the accuracy on these new predictions, and compare the accuracy to the accuracy on the test set (OPTIONAL: Discuss the results in more detail as described in the \"Stand Out Suggestions\" part of the rubric).\n",
    "\n",
    "The code for making predictions on my final model is in IPython notebook under the heading \"Predict the Sign Type\" for Each Image\n",
    "\n",
    "Here are my results for prediction:\n",
    "\n",
    "    image\t                                Prediction\n",
    "    -----------------------------------------------------------------------------\n",
    "    Priority road\t                        Priority road\n",
    "    Turn left ahead\t                      Turn left ahead\n",
    "    Speed limit (60km/h)\t                 Speed limit (60km/h)\n",
    "    Right-of-way at the next intersection\tRight-of-way at the next intersection\n",
    "    Speed limit (20km/h)                     Speed limit (20km/h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my model wcould predict correctly all five images of traffic signs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Describe how certain the model is when predicting on each of the five new images by looking at the softmax probabilities for each prediction and identify where in your code softmax probabilities were outputted. Provide the top 5 softmax probabilities for each image along with the sign type of each probability. (OPTIONAL: as described in the \"Stand Out Suggestions\" part of the rubric, visualizations can also be provided such as bar charts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./prob_hist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for all of these five images, the model could get the probability of 1 for correct class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Required:\n",
    "\n",
    "###### You provide a good analysis of the results on the newly acquired images but forgot to compare the accuracy on the new set of images to that on the old test set and tell if your model is overfitting or underfitting.\n",
    "\n",
    "The accuracy on the captured images(5 images that I fed to model) is 100% while it was:\n",
    "    1. training set accuracy of 99.9%\n",
    "    2. validation set accuracy of 95.3%\n",
    "    3. test set accuracy of 93.1%\n",
    "    \n",
    "therefore It seems the model is working well on chosen images, Since the test accuracy(93.1) is less than validation accuracy(95.3%) and the result I got for these images(100%) is higher, so the model can be improved in the future by adding more layers and tunning better hyperparameters and adding regularization, so it seems the model is slightly overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Just a little thing missing : You should clearly discuss how certain or uncertain your model is of its prediction.\n",
    "\n",
    "I run model on five images that I chose, and here is the result of \n",
    "\n",
    "my_top_k = sess.run(top_k, feed_dict={x: images_test, keep_prob: 1.})\n",
    "\n",
    "    TopKV2(values=array([\n",
    "    [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00, 0.00000000e+00,   0.00000000e+00],\n",
    "    [  1.00000000e+00,   1.12621469e-15,   2.65482870e-21, 3.89836245e-32,   4.56915729e-38],\n",
    "    [  1.00000000e+00,   3.79276299e-09,   9.29154739e-11, 6.39640355e-11,   8.53514850e-13],\n",
    "    [  1.00000000e+00,   2.14781445e-21,   0.00000000e+00, 0.00000000e+00,   0.00000000e+00],\n",
    "    [  1.00000000e+00,   3.67713319e-19,   4.75852056e-22, 2.57694976e-36,   0.00000000e+00]], dtype=float32), \n",
    "\n",
    "    indices=array([[12,  0,  1,  2,  3],\n",
    "                   [34, 38, 36, 13, 40],\n",
    "                   [ 3,  5,  2, 25, 10],\n",
    "                   [11, 30,  0,  1,  2],\n",
    "                   [ 1,  5,  2, 38,  0]], dtype=int32))\n",
    "               \n",
    "               \n",
    "Looking just at the first row we get [1.00000000e+00,0.00000000e+00,0.00000000e+00, 0.00000000e+00,0.00000000e+00], we can confirm these are the 5 largest probabilities. You'll also notice [12,  0,  1,  2,  3] are the corresponding indices. As it is clear, for all of these five images, the highest probability is 1, so that is why it gave me accuracy of 100%. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
